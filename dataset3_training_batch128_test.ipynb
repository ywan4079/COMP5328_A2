{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0861f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ImageDataset, load_dataset, train_val_split, data_augmentation\n",
    "from model import CNN, CNNWithNAL\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc18ed",
   "metadata": {},
   "source": [
    "# CIFAR\n",
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217a6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels, testing_data, testing_labels = load_dataset('datasets/CIFAR.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf544d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6274dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subset, training_sub_labels, validation_subset, validation_sub_labels = train_val_split(training_data, training_labels)\n",
    "aug_training, aug_labels = data_augmentation(training_subset, training_sub_labels)\n",
    "\n",
    "train_dataset = ImageDataset(training_subset, training_sub_labels)\n",
    "val_dataset = ImageDataset(validation_subset, validation_sub_labels)\n",
    "test_dataset = ImageDataset(testing_data, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8eade4",
   "metadata": {},
   "source": [
    "## CNN with Noise Adaption Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2b7eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Training CNN round 1/10----------\n",
      "Training noisy CNN to estimate NAL Layer params...\n",
      "Epoch [1/100], Training Loss: 1.1693, Validation Loss: 1.1634, Validation Accuracy: 31.97%\n",
      "Epoch [2/100], Training Loss: 1.0822, Validation Loss: 1.1325, Validation Accuracy: 35.03%\n",
      "Epoch [3/100], Training Loss: 1.0452, Validation Loss: 1.1654, Validation Accuracy: 35.47%\n",
      "Epoch [4/100], Training Loss: 0.9973, Validation Loss: 1.1948, Validation Accuracy: 35.33%\n",
      "Epoch [5/100], Training Loss: 0.8859, Validation Loss: 1.4590, Validation Accuracy: 34.23%\n",
      "Epoch [6/100], Training Loss: 0.7152, Validation Loss: 1.4837, Validation Accuracy: 36.10%\n",
      "Epoch [7/100], Training Loss: 0.4795, Validation Loss: 2.1679, Validation Accuracy: 33.90%\n",
      "No improvement for 5 epochs. Early stopping.\n",
      "Epoch [1/100], Training Loss: 1.1339, Validation Loss: 1.1134, Validation Accuracy: 36.47%\n",
      "Epoch [2/100], Training Loss: 1.0821, Validation Loss: 1.1014, Validation Accuracy: 37.50%\n",
      "Epoch [3/100], Training Loss: 1.0550, Validation Loss: 1.1301, Validation Accuracy: 33.60%\n",
      "Epoch [4/100], Training Loss: 1.0172, Validation Loss: 1.1957, Validation Accuracy: 34.10%\n",
      "Epoch [5/100], Training Loss: 0.9517, Validation Loss: 1.2080, Validation Accuracy: 33.40%\n",
      "Epoch [6/100], Training Loss: 0.8332, Validation Loss: 1.3620, Validation Accuracy: 35.73%\n",
      "Epoch [7/100], Training Loss: 0.7213, Validation Loss: 1.4565, Validation Accuracy: 34.00%\n",
      "No improvement for 5 epochs. Early stopping.\n",
      "CNN Test Acc: 35.73%\n",
      "----------Training CNN round 2/10----------\n",
      "Training noisy CNN to estimate NAL Layer params...\n",
      "Epoch [1/100], Training Loss: 1.1445, Validation Loss: 1.1241, Validation Accuracy: 34.97%\n",
      "Epoch [2/100], Training Loss: 1.0860, Validation Loss: 1.1011, Validation Accuracy: 37.27%\n",
      "Epoch [3/100], Training Loss: 1.0585, Validation Loss: 1.1209, Validation Accuracy: 37.57%\n",
      "Epoch [4/100], Training Loss: 1.0162, Validation Loss: 1.1289, Validation Accuracy: 37.03%\n",
      "Epoch [5/100], Training Loss: 0.9360, Validation Loss: 1.1893, Validation Accuracy: 35.10%\n",
      "Epoch [6/100], Training Loss: 0.8190, Validation Loss: 1.4171, Validation Accuracy: 34.73%\n",
      "Epoch [7/100], Training Loss: 0.6339, Validation Loss: 1.6893, Validation Accuracy: 35.17%\n",
      "No improvement for 5 epochs. Early stopping.\n",
      "Epoch [1/100], Training Loss: 1.1170, Validation Loss: 1.0928, Validation Accuracy: 36.60%\n",
      "Epoch [2/100], Training Loss: 1.0891, Validation Loss: 1.0908, Validation Accuracy: 35.17%\n",
      "Epoch [3/100], Training Loss: 1.0821, Validation Loss: 1.0961, Validation Accuracy: 34.87%\n",
      "Epoch [4/100], Training Loss: 1.0668, Validation Loss: 1.1225, Validation Accuracy: 33.97%\n",
      "Epoch [5/100], Training Loss: 1.0457, Validation Loss: 1.1068, Validation Accuracy: 36.27%\n",
      "Epoch [6/100], Training Loss: 1.0114, Validation Loss: 1.1791, Validation Accuracy: 36.60%\n",
      "Epoch [7/100], Training Loss: 0.9303, Validation Loss: 1.2782, Validation Accuracy: 35.67%\n",
      "No improvement for 5 epochs. Early stopping.\n",
      "CNN Test Acc: 46.27%\n",
      "----------Training CNN round 3/10----------\n",
      "Training noisy CNN to estimate NAL Layer params...\n",
      "Epoch [1/100], Training Loss: 1.1516, Validation Loss: 1.4530, Validation Accuracy: 36.50%\n",
      "Epoch [2/100], Training Loss: 1.0929, Validation Loss: 1.1535, Validation Accuracy: 32.13%\n",
      "Epoch [3/100], Training Loss: 1.0688, Validation Loss: 1.1544, Validation Accuracy: 34.30%\n",
      "Epoch [4/100], Training Loss: 1.0276, Validation Loss: 1.1505, Validation Accuracy: 36.63%\n",
      "Epoch [5/100], Training Loss: 0.9708, Validation Loss: 1.2027, Validation Accuracy: 35.70%\n",
      "Epoch [6/100], Training Loss: 0.8453, Validation Loss: 1.2807, Validation Accuracy: 33.97%\n",
      "Epoch [7/100], Training Loss: 0.6784, Validation Loss: 1.5873, Validation Accuracy: 34.57%\n",
      "Epoch [8/100], Training Loss: 0.4776, Validation Loss: 2.3389, Validation Accuracy: 33.83%\n",
      "Epoch [9/100], Training Loss: 0.2807, Validation Loss: 2.4678, Validation Accuracy: 33.97%\n",
      "No improvement for 5 epochs. Early stopping.\n",
      "Epoch [1/100], Training Loss: 1.1495, Validation Loss: 1.1941, Validation Accuracy: 35.30%\n",
      "Epoch [2/100], Training Loss: 1.0849, Validation Loss: 1.1024, Validation Accuracy: 38.27%\n",
      "Epoch [3/100], Training Loss: 1.0571, Validation Loss: 1.1056, Validation Accuracy: 37.80%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m----------Training CNN round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10----------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m cnn = CNNWithNAL(num_classes=\u001b[32m3\u001b[39m, dataset_name=\u001b[33m\"\u001b[39m\u001b[33mCIFAR\u001b[39m\u001b[33m\"\u001b[39m, batch_size=\u001b[32m256\u001b[39m, learning_rate=\u001b[32m0.0005\u001b[39m, patience=\u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mcnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m y_true, y_pred = cnn.predict(test_dataset)\n\u001b[32m     10\u001b[39m prediction_results.append((y_true, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ronald\\Desktop\\COMP5328_A2\\model.py:244\u001b[39m, in \u001b[36mCNNWithNAL.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.nal.logits = \u001b[38;5;28mself\u001b[39m.get_NAL_params(noisy_cnn, train_dataset)\n\u001b[32m    242\u001b[39m \u001b[38;5;28mself\u001b[39m.nal.logits = \u001b[38;5;28mself\u001b[39m.nal.logits.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnal_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ronald\\Desktop\\COMP5328_A2\\model.py:99\u001b[39m, in \u001b[36mModelBase.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, nal_layer)\u001b[39m\n\u001b[32m     97\u001b[39m     loss.backward()\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m training_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('results/cnnwithnal_CIFAR_pred_results.pkl'):\n",
    "    prediction_results = []\n",
    "    accuracy_results = []\n",
    "    for round in range(10):\n",
    "        print(f\"----------Training CNN round {round+1}/10----------\")\n",
    "        cnn = CNNWithNAL(num_classes=3, dataset_name=\"CIFAR\", batch_size=256, learning_rate=0.0005, patience=5)\n",
    "        cnn.train(train_dataset, val_dataset)\n",
    "        y_true, y_pred = cnn.predict(test_dataset)\n",
    "        prediction_results.append((y_true, y_pred))\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_results.append(accuracy)\n",
    "        print(f\"CNN Test Acc: {accuracy*100:.2f}%\")\n",
    "\n",
    "    with open('results/cnnwithnal_CIFAR_pred_results.pkl', 'wb') as f:\n",
    "        pickle.dump(prediction_results, f)\n",
    "\n",
    "    with open('results/cnnwithnal_CIFAR_acc_results.pkl', 'wb') as f:\n",
    "        pickle.dump(accuracy_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
